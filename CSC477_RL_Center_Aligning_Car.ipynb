{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "07qYHj7mYTvZ",
        "eysonnWaY0oG",
        "MUR8XcygZJa_",
        "EFqHAtHWZap0"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Initialization"
      ],
      "metadata": {
        "id": "iGD16IBnXMiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Requires restart\n",
        "!pip install \"numpy<1.24\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "VY4yjVt7Xq73",
        "outputId": "99d1d67e-3f5f-4bae-dfe4-d87864a593b1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy<1.24\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "pandas-stubs 2.0.3.230814 requires numpy>=1.25.0; python_version >= \"3.9\", but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "ea59c82f16814578a11f720cf2465d21"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z14t4hpzXACX",
        "outputId": "83530e75-e0b0-4739-e2f1-f5841794bbcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: swig in /usr/local/lib/python3.10/dist-packages (4.2.1)\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.10.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.2.1)\n",
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.3.0-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.1/182.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.2.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (4.10.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.13.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13->stable-baselines3)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13->stable-baselines3)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13->stable-baselines3)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13->stable-baselines3)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13->stable-baselines3)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13->stable-baselines3)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13->stable-baselines3)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13->stable-baselines3)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13->stable-baselines3)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.13->stable-baselines3)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13->stable-baselines3)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable-baselines3)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable-baselines3) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable-baselines3\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 stable-baselines3-2.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install swig\n",
        "!pip install gymnasium[box2d]\n",
        "!pip install stable-baselines3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from math import atan2, cos, sin, sqrt\n",
        "from typing import Optional, Union\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from gymnasium.envs.box2d.car_dynamics import Car\n",
        "from gymnasium.error import DependencyNotInstalled, InvalidAction\n",
        "from gymnasium.utils import EzPickle\n",
        "\n",
        "import csv\n",
        "import time\n",
        "import os\n",
        "\n",
        "SEED = 0\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import relu\n",
        "\n",
        "from stable_baselines3 import PPO, SAC, A2C, DDPG\n",
        "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
        "from stable_baselines3.common.logger import configure\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "I9Biu28rYFDc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Environment"
      ],
      "metadata": {
        "id": "07qYHj7mYTvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__credits__ = [\"Andrea PIERRÉ\"]\n",
        "\n",
        "\n",
        "try:\n",
        "    import Box2D\n",
        "    from Box2D.b2 import contactListener, fixtureDef, polygonShape\n",
        "except ImportError as e:\n",
        "    raise DependencyNotInstalled(\n",
        "        \"Box2D is not installed, run `pip install gymnasium[box2d]`\"\n",
        "    ) from e\n",
        "\n",
        "try:\n",
        "    # As pygame is necessary for using the environment (reset and step) even without a render mode\n",
        "    #   therefore, pygame is a necessary import for the environment.\n",
        "    import pygame\n",
        "    from pygame import gfxdraw\n",
        "except ImportError as e:\n",
        "    raise DependencyNotInstalled(\n",
        "        \"pygame is not installed, run `pip install gymnasium[box2d]`\"\n",
        "    ) from e\n",
        "\n",
        "\n",
        "STATE_W = 96  # less than Atari 160x192\n",
        "STATE_H = 96\n",
        "VIDEO_W = 600\n",
        "VIDEO_H = 400\n",
        "WINDOW_W = 800\n",
        "WINDOW_H = 600\n",
        "\n",
        "SCALE = 6.0  # Track scale\n",
        "TRACK_RAD = 900 / SCALE  # Track is heavily morphed circle with this radius\n",
        "PLAYFIELD = 2000 / SCALE  # Game over boundary\n",
        "FPS = 50  # Frames per second\n",
        "ZOOM = 2.7  # Camera zoom\n",
        "ZOOM_FOLLOW = True  # Set to False for fixed view (don't use zoom)\n",
        "\n",
        "\n",
        "TRACK_DETAIL_STEP = 21 / SCALE\n",
        "TRACK_TURN_RATE = 0.31\n",
        "TRACK_WIDTH = 40 / SCALE\n",
        "BORDER = 8 / SCALE\n",
        "BORDER_MIN_COUNT = 4\n",
        "GRASS_DIM = PLAYFIELD / 20.0\n",
        "MAX_SHAPE_DIM = (\n",
        "    max(GRASS_DIM, TRACK_WIDTH, TRACK_DETAIL_STEP) * math.sqrt(2) * ZOOM * SCALE\n",
        ")\n",
        "\n",
        "#Prev errors for CTE variance calc\n",
        "NUM_PREV_ERRORS = 20\n",
        "\n",
        "# Reward constants\n",
        "CTE_RESCALE = 200\n",
        "REWARD_VSHIFT = 10\n",
        "OFF_ROAD_PENALTY = -1000\n",
        "\n",
        "VARIABLE_SPEED ={\"On\" : True, \"min_speed\": 30, \"max_speed\": 70}\n",
        "\n",
        "'''https://www.desmos.com/calculator/dtotkkusih'''\n",
        "\n",
        "class FrictionDetector(contactListener):\n",
        "    def __init__(self, env, lap_complete_percent):\n",
        "        contactListener.__init__(self)\n",
        "        self.env = env\n",
        "        self.lap_complete_percent = lap_complete_percent\n",
        "\n",
        "    def BeginContact(self, contact):\n",
        "        self._contact(contact, True)\n",
        "\n",
        "    def EndContact(self, contact):\n",
        "        self._contact(contact, False)\n",
        "\n",
        "    def _contact(self, contact, begin):\n",
        "        tile = None\n",
        "        obj = None\n",
        "        u1 = contact.fixtureA.body.userData\n",
        "        u2 = contact.fixtureB.body.userData\n",
        "        if u1 and \"road_friction\" in u1.__dict__:\n",
        "            tile = u1\n",
        "            obj = u2\n",
        "        if u2 and \"road_friction\" in u2.__dict__:\n",
        "            tile = u2\n",
        "            obj = u1\n",
        "        if not tile:\n",
        "            return\n",
        "\n",
        "        # inherit tile color from env\n",
        "        tile.color[:] = self.env.road_color\n",
        "        if not obj or \"tiles\" not in obj.__dict__:\n",
        "            return\n",
        "        if begin:\n",
        "            obj.tiles.add(tile)\n",
        "            if not tile.road_visited:\n",
        "                tile.road_visited = True\n",
        "                self.env.reward += 0 #/ len(self.env.track)\n",
        "                self.env.tile_visited_count += 1\n",
        "\n",
        "                # Lap is considered completed if enough % of the track was covered\n",
        "                if (#tile.idx == 0 and\n",
        "                    (self.env.tile_visited_count / len(self.env.track)) > self.lap_complete_percent):\n",
        "                    self.env.new_lap = True\n",
        "        else:\n",
        "            obj.tiles.remove(tile)\n",
        "\n",
        "\n",
        "class CarRacing(gym.Env, EzPickle):\n",
        "    \"\"\"\n",
        "    ## Description\n",
        "    The easiest control task to learn from pixels - a top-down\n",
        "    racing environment. The generated track is random every episode.\n",
        "\n",
        "    Some indicators are shown at the bottom of the window along with the\n",
        "    state RGB buffer. From left to right: true speed, four ABS sensors,\n",
        "    steering wheel position, and gyroscope.\n",
        "    To play yourself (it's rather fast for humans), type:\n",
        "    ```shell\n",
        "    python gymnasium/envs/box2d/car_racing.py\n",
        "    ```\n",
        "    Remember: it's a powerful rear-wheel drive car - don't press the accelerator\n",
        "    and turn at the same time.\n",
        "\n",
        "    ## Action Space\n",
        "    If continuous there are 3 actions :\n",
        "    - 0: steering, -1 is full left, +1 is full right\n",
        "    - 1: gas\n",
        "    - 2: breaking\n",
        "\n",
        "    If discrete there are 5 actions:\n",
        "    - 0: do nothing\n",
        "    - 1: steer left\n",
        "    - 2: steer right\n",
        "    - 3: gas\n",
        "    - 4: brake\n",
        "\n",
        "    ## Observation Space\n",
        "\n",
        "    A top-down 96x96 RGB image of the car and race track.\n",
        "\n",
        "    ## Rewards\n",
        "    The reward is -0.1 every frame and +1000/N for every track tile visited, where N is the total number of tiles\n",
        "     visited in the track. For example, if you have finished in 732 frames, your reward is 1000 - 0.1*732 = 926.8 points.\n",
        "\n",
        "    ## Starting State\n",
        "    The car starts at rest in the center of the road.\n",
        "\n",
        "    ## Episode Termination\n",
        "    The episode finishes when all the tiles are visited. The car can also go outside the playfield -\n",
        "     that is, far off the track, in which case it will receive -100 reward and die.\n",
        "\n",
        "    ## Arguments\n",
        "\n",
        "    ```python\n",
        "    >>> import gymnasium as gym\n",
        "    >>> env = gym.make(\"CarRacing-v2\", render_mode=\"rgb_array\", lap_complete_percent=0.95, domain_randomize=False, continuous=False)\n",
        "    >>> env\n",
        "    <TimeLimit<OrderEnforcing<PassiveEnvChecker<CarRacing<CarRacing-v2>>>>>\n",
        "\n",
        "    ```\n",
        "\n",
        "    * `lap_complete_percent=0.95` dictates the percentage of tiles that must be visited by\n",
        "     the agent before a lap is considered complete.\n",
        "\n",
        "    * `domain_randomize=False` enables the domain randomized variant of the environment.\n",
        "     In this scenario, the background and track colours are different on every reset.\n",
        "\n",
        "    * `continuous=True` converts the environment to use discrete action space.\n",
        "     The discrete action space has 5 actions: [do nothing, left, right, gas, brake].\n",
        "\n",
        "    ## Reset Arguments\n",
        "\n",
        "    Passing the option `options[\"randomize\"] = True` will change the current colour of the environment on demand.\n",
        "    Correspondingly, passing the option `options[\"randomize\"] = False` will not change the current colour of the environment.\n",
        "    `domain_randomize` must be `True` on init for this argument to work.\n",
        "\n",
        "    ```python\n",
        "    >>> import gymnasium as gym\n",
        "    >>> env = gym.make(\"CarRacing-v2\", domain_randomize=True)\n",
        "\n",
        "    # normal reset, this changes the colour scheme by default\n",
        "    >>> obs, _ = env.reset()\n",
        "\n",
        "    # reset with colour scheme change\n",
        "    >>> randomize_obs, _ = env.reset(options={\"randomize\": True})\n",
        "\n",
        "    # reset with no colour scheme change\n",
        "    >>> non_random_obs, _ = env.reset(options={\"randomize\": False})\n",
        "\n",
        "    ```\n",
        "\n",
        "    ## Version History\n",
        "    - v1: Change track completion logic and add domain randomization (0.24.0)\n",
        "    - v0: Original version\n",
        "\n",
        "    ## References\n",
        "    - Chris Campbell (2014), http://www.iforce2d.net/b2dtut/top-down-car.\n",
        "\n",
        "    ## Credits\n",
        "    Created by Oleg Klimov\n",
        "    \"\"\"\n",
        "\n",
        "    metadata = {\n",
        "        \"render_modes\": [\n",
        "            \"human\",\n",
        "            \"rgb_array\",\n",
        "            \"state_pixels\",\n",
        "        ],\n",
        "        \"render_fps\": FPS,\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        render_mode: Optional[str] = None,\n",
        "        verbose: bool = False,\n",
        "        lap_complete_percent: float = 1.0,\n",
        "        domain_randomize: bool = False,\n",
        "        continuous: bool = True,\n",
        "        constant_speed = 50,\n",
        "        num_prev_errors = NUM_PREV_ERRORS\n",
        "    ):\n",
        "        EzPickle.__init__(\n",
        "            self,\n",
        "            render_mode,\n",
        "            verbose,\n",
        "            lap_complete_percent,\n",
        "            domain_randomize,\n",
        "            continuous,\n",
        "        )\n",
        "        self.continuous = continuous\n",
        "        self.domain_randomize = domain_randomize\n",
        "        self.lap_complete_percent = lap_complete_percent\n",
        "        self._init_colors()\n",
        "\n",
        "\n",
        "        self.center_line = []\n",
        "        self._max_episode_steps = 2500\n",
        "        self.episode_steps = 0\n",
        "        self.constant_speed = constant_speed\n",
        "        self.prev_errors = [0 for _ in range(num_prev_errors)]\n",
        "        self.road_half_width = 7\n",
        "        self.placeholder = 0\n",
        "\n",
        "\n",
        "        self.contactListener_keepref = FrictionDetector(self, self.lap_complete_percent)\n",
        "        self.world = Box2D.b2World((0, 0), contactListener=self.contactListener_keepref)\n",
        "        self.screen: Optional[pygame.Surface] = None\n",
        "        self.surf = None\n",
        "        self.clock = None\n",
        "        self.isopen = True\n",
        "        self.invisible_state_window = None\n",
        "        self.invisible_video_window = None\n",
        "        self.road = None\n",
        "        self.car: Optional[Car] = None\n",
        "        self.reward = 0.0\n",
        "        self.prev_reward = 0.0\n",
        "        self.verbose = verbose\n",
        "        self.new_lap = False\n",
        "        self.fd_tile = fixtureDef(\n",
        "            shape=polygonShape(vertices=[(0, 0), (1, 0), (1, -1), (0, -1)])\n",
        "        )\n",
        "\n",
        "        # This will throw a warning in tests/envs/test_envs in utils/env_checker.py as the space is not symmetric\n",
        "        #   or normalised however this is not possible here so ignore\n",
        "        if self.continuous:\n",
        "            self.action_space = spaces.Box(\n",
        "                np.array([-1]).astype(np.float64),\n",
        "                np.array([+1]).astype(np.float64),\n",
        "                seed = SEED\n",
        "\n",
        "                #np.array([-1, 0, 0]).astype(np.float32),\n",
        "                #np.array([+1, +1, +1]).astype(np.float32),\n",
        "            )  # steer, gas, brake\n",
        "        else:\n",
        "            self.action_space = spaces.Discrete(2)\n",
        "            # only steer left and right\n",
        "\n",
        "        # obseravtion: [error_heading, CTE, Speed]\n",
        "        if not VARIABLE_SPEED['On']:\n",
        "            self.observation_space = spaces.Box(\n",
        "                np.array([ -1, -1]).astype(np.float64),\n",
        "                np.array([+1, +1]).astype(np.float64), seed = SEED)\n",
        "        else:\n",
        "            self.observation_space = spaces.Box(\n",
        "                np.array([ -1, -1, 0]).astype(np.float64),\n",
        "                np.array([+1, +1, 1]).astype(np.float64), seed = SEED)\n",
        "\n",
        "\n",
        "        '''\n",
        "            np.array([ -math.pi, -WINDOW_W, -2*self.road_half_width]).astype(np.float64),\n",
        "            np.array([+math.pi, +WINDOW_W, +2*self.road_half_width]).astype(np.float64), seed = SEED)'''\n",
        "\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "    def _destroy(self):\n",
        "        if not self.road:\n",
        "            return\n",
        "        for t in self.road:\n",
        "            self.world.DestroyBody(t)\n",
        "        self.road = []\n",
        "        assert self.car is not None\n",
        "        self.car.destroy()\n",
        "\n",
        "    def _init_colors(self):\n",
        "        if self.domain_randomize:\n",
        "            # domain randomize the bg and grass colour\n",
        "            self.road_color = self.np_random.uniform(0, 210, size=3)\n",
        "\n",
        "            self.bg_color = self.np_random.uniform(0, 210, size=3)\n",
        "\n",
        "            self.grass_color = np.copy(self.bg_color)\n",
        "            idx = self.np_random.integers(3)\n",
        "            self.grass_color[idx] += 20\n",
        "        else:\n",
        "            # default colours\n",
        "            self.road_color = np.array([57, 64, 83])\n",
        "            self.bg_color = np.array([110, 99, 98])\n",
        "            self.grass_color = np.array([131, 144, 115])\n",
        "\n",
        "    def _reinit_colors(self, randomize):\n",
        "        assert (\n",
        "            self.domain_randomize\n",
        "        ), \"domain_randomize must be True to use this function.\"\n",
        "\n",
        "        if randomize:\n",
        "            # domain randomize the bg and grass colour\n",
        "            self.road_color = self.np_random.uniform(0, 210, size=3)\n",
        "\n",
        "            self.bg_color = self.np_random.uniform(0, 210, size=3)\n",
        "\n",
        "            self.grass_color = np.copy(self.bg_color)\n",
        "            idx = self.np_random.integers(3)\n",
        "            self.grass_color[idx] += 20\n",
        "\n",
        "    def _create_track(self):\n",
        "        CHECKPOINTS = 12\n",
        "\n",
        "        # Create checkpoints\n",
        "        checkpoints = []\n",
        "        for c in range(CHECKPOINTS):\n",
        "            noise = self.np_random.uniform(0, 2 * math.pi * 1 / CHECKPOINTS)\n",
        "            alpha = 2 * math.pi * c / CHECKPOINTS + noise\n",
        "            rad = self.np_random.uniform(TRACK_RAD / 3, TRACK_RAD)\n",
        "\n",
        "            if c == 0:\n",
        "                alpha = 0\n",
        "                rad = 1.5 * TRACK_RAD\n",
        "            if c == CHECKPOINTS - 1:\n",
        "                alpha = 2 * math.pi * c / CHECKPOINTS\n",
        "                self.start_alpha = 2 * math.pi * (-0.5) / CHECKPOINTS\n",
        "                rad = 1.5 * TRACK_RAD\n",
        "\n",
        "            checkpoints.append((alpha, rad * math.cos(alpha), rad * math.sin(alpha)))\n",
        "        self.road = []\n",
        "\n",
        "        # Go from one checkpoint to another to create track\n",
        "        x, y, beta = 1.5 * TRACK_RAD, 0, 0\n",
        "\n",
        "        dest_i = 0\n",
        "        laps = 0\n",
        "        track = []\n",
        "        no_freeze = 2500\n",
        "        visited_other_side = False\n",
        "        while True:\n",
        "            alpha = math.atan2(y, x)\n",
        "            if visited_other_side and alpha > 0:\n",
        "                laps += 1\n",
        "                visited_other_side = False\n",
        "            if alpha < 0:\n",
        "                visited_other_side = True\n",
        "                alpha += 2 * math.pi\n",
        "\n",
        "            while True:  # Find destination from checkpoints\n",
        "                failed = True\n",
        "\n",
        "                while True:\n",
        "                    dest_alpha, dest_x, dest_y = checkpoints[dest_i % len(checkpoints)]\n",
        "                    if alpha <= dest_alpha:\n",
        "                        failed = False\n",
        "                        break\n",
        "                    dest_i += 1\n",
        "                    if dest_i % len(checkpoints) == 0:\n",
        "                        break\n",
        "\n",
        "                if not failed:\n",
        "                    break\n",
        "\n",
        "                alpha -= 2 * math.pi\n",
        "                continue\n",
        "\n",
        "            r1x = math.cos(beta)\n",
        "            r1y = math.sin(beta)\n",
        "            p1x = -r1y\n",
        "            p1y = r1x\n",
        "            dest_dx = dest_x - x  # vector towards destination\n",
        "            dest_dy = dest_y - y\n",
        "            # destination vector projected on rad:\n",
        "            proj = r1x * dest_dx + r1y * dest_dy\n",
        "            while beta - alpha > 1.5 * math.pi:\n",
        "                beta -= 2 * math.pi\n",
        "            while beta - alpha < -1.5 * math.pi:\n",
        "                beta += 2 * math.pi\n",
        "            prev_beta = beta\n",
        "            proj *= SCALE\n",
        "            if proj > 0.3:\n",
        "                beta -= min(TRACK_TURN_RATE, abs(0.001 * proj))\n",
        "            if proj < -0.3:\n",
        "                beta += min(TRACK_TURN_RATE, abs(0.001 * proj))\n",
        "            x += p1x * TRACK_DETAIL_STEP\n",
        "            y += p1y * TRACK_DETAIL_STEP\n",
        "            track.append((alpha, prev_beta * 0.5 + beta * 0.5, x, y))\n",
        "\n",
        "            self.center_line.append((x,y))\n",
        "\n",
        "            if laps > 4:\n",
        "                break\n",
        "            no_freeze -= 1\n",
        "            if no_freeze == 0:\n",
        "                break\n",
        "\n",
        "        # Find closed loop range i1..i2, first loop should be ignored, second is OK\n",
        "        i1, i2 = -1, -1\n",
        "        i = len(track)\n",
        "        while True:\n",
        "            i -= 1\n",
        "            if i == 0:\n",
        "                return False  # Failed\n",
        "            pass_through_start = (\n",
        "                track[i][0] > self.start_alpha and track[i - 1][0] <= self.start_alpha\n",
        "            )\n",
        "            if pass_through_start and i2 == -1:\n",
        "                i2 = i\n",
        "            elif pass_through_start and i1 == -1:\n",
        "                i1 = i\n",
        "                break\n",
        "        if self.verbose:\n",
        "            print(\"Track generation: %i..%i -> %i-tiles track\" % (i1, i2, i2 - i1))\n",
        "        assert i1 != -1\n",
        "        assert i2 != -1\n",
        "\n",
        "        track = track[i1 : i2 - 1]\n",
        "\n",
        "        first_beta = track[0][1]\n",
        "        first_perp_x = math.cos(first_beta)\n",
        "        first_perp_y = math.sin(first_beta)\n",
        "        # Length of perpendicular jump to put together head and tail\n",
        "        well_glued_together = np.sqrt(\n",
        "            np.square(first_perp_x * (track[0][2] - track[-1][2]))\n",
        "            + np.square(first_perp_y * (track[0][3] - track[-1][3]))\n",
        "        )\n",
        "        if well_glued_together > TRACK_DETAIL_STEP:\n",
        "            return False\n",
        "\n",
        "        # Red-white border on hard turns\n",
        "        border = [False] * len(track)\n",
        "        for i in range(len(track)):\n",
        "            good = True\n",
        "            oneside = 0\n",
        "            for neg in range(BORDER_MIN_COUNT):\n",
        "                beta1 = track[i - neg - 0][1]\n",
        "                beta2 = track[i - neg - 1][1]\n",
        "                good &= abs(beta1 - beta2) > TRACK_TURN_RATE * 0.2\n",
        "                oneside += np.sign(beta1 - beta2)\n",
        "            good &= abs(oneside) == BORDER_MIN_COUNT\n",
        "            border[i] = good\n",
        "        for i in range(len(track)):\n",
        "            for neg in range(BORDER_MIN_COUNT):\n",
        "                border[i - neg] |= border[i]\n",
        "\n",
        "        # Create tiles\n",
        "        for i in range(len(track)):\n",
        "            alpha1, beta1, x1, y1 = track[i]\n",
        "            alpha2, beta2, x2, y2 = track[i - 1]\n",
        "            road1_l = (\n",
        "                x1 - TRACK_WIDTH * math.cos(beta1),\n",
        "                y1 - TRACK_WIDTH * math.sin(beta1),\n",
        "            )\n",
        "            road1_r = (\n",
        "                x1 + TRACK_WIDTH * math.cos(beta1),\n",
        "                y1 + TRACK_WIDTH * math.sin(beta1),\n",
        "            )\n",
        "            road2_l = (\n",
        "                x2 - TRACK_WIDTH * math.cos(beta2),\n",
        "                y2 - TRACK_WIDTH * math.sin(beta2),\n",
        "            )\n",
        "            road2_r = (\n",
        "                x2 + TRACK_WIDTH * math.cos(beta2),\n",
        "                y2 + TRACK_WIDTH * math.sin(beta2),\n",
        "            )\n",
        "            vertices = [road1_l, road1_r, road2_r, road2_l]\n",
        "            self.fd_tile.shape.vertices = vertices\n",
        "            t = self.world.CreateStaticBody(fixtures=self.fd_tile)\n",
        "            t.userData = t\n",
        "            c = 0.01 * (i % 3) * 255\n",
        "            t.color = self.road_color + c\n",
        "            t.road_visited = False\n",
        "            t.road_friction = 1.0\n",
        "            t.idx = i\n",
        "            t.fixtures[0].sensor = True\n",
        "            self.road_poly.append(([road1_l, road1_r, road2_r, road2_l], t.color))\n",
        "            self.road.append(t)\n",
        "            if border[i]:\n",
        "                side = np.sign(beta2 - beta1)\n",
        "                b1_l = (\n",
        "                    x1 + side * TRACK_WIDTH * math.cos(beta1),\n",
        "                    y1 + side * TRACK_WIDTH * math.sin(beta1),\n",
        "                )\n",
        "                b1_r = (\n",
        "                    x1 + side * (TRACK_WIDTH + BORDER) * math.cos(beta1),\n",
        "                    y1 + side * (TRACK_WIDTH + BORDER) * math.sin(beta1),\n",
        "                )\n",
        "                b2_l = (\n",
        "                    x2 + side * TRACK_WIDTH * math.cos(beta2),\n",
        "                    y2 + side * TRACK_WIDTH * math.sin(beta2),\n",
        "                )\n",
        "                b2_r = (\n",
        "                    x2 + side * (TRACK_WIDTH + BORDER) * math.cos(beta2),\n",
        "                    y2 + side * (TRACK_WIDTH + BORDER) * math.sin(beta2),\n",
        "                )\n",
        "                self.road_poly.append(\n",
        "                    (\n",
        "                        [b1_l, b1_r, b2_r, b2_l],\n",
        "                        (255, 255, 255) if i % 2 == 0 else (255, 0, 0),\n",
        "                    )\n",
        "                )\n",
        "        self.track = track\n",
        "        return True\n",
        "\n",
        "    def reset(\n",
        "        self,\n",
        "        *,\n",
        "        seed: Optional[int] = None,\n",
        "        options: Optional[dict] = None,\n",
        "    ):\n",
        "        super().reset(seed=seed)\n",
        "        self._destroy()\n",
        "        self.world.contactListener_bug_workaround = FrictionDetector(\n",
        "            self, self.lap_complete_percent\n",
        "        )\n",
        "        self.world.contactListener = self.world.contactListener_bug_workaround\n",
        "        self.reward = 0.0\n",
        "        self.prev_reward = 0.0\n",
        "        self.tile_visited_count = 0\n",
        "        self.t = 0.0\n",
        "        self.new_lap = False\n",
        "        self.road_poly = []\n",
        "\n",
        "\n",
        "        ################\n",
        "        if VARIABLE_SPEED['On']:\n",
        "            self.constant_speed = np.random.uniform(VARIABLE_SPEED['min_speed'], VARIABLE_SPEED[\"max_speed\"])\n",
        "\n",
        "        self.episode_steps = 0\n",
        "        ##############\n",
        "\n",
        "        if self.domain_randomize:\n",
        "            randomize = True\n",
        "            if isinstance(options, dict):\n",
        "                if \"randomize\" in options:\n",
        "                    randomize = options[\"randomize\"]\n",
        "\n",
        "            self._reinit_colors(randomize)\n",
        "\n",
        "        while True:\n",
        "            success = self._create_track()\n",
        "            if success:\n",
        "                break\n",
        "            if self.verbose:\n",
        "                print(\n",
        "                    \"retry to generate track (normal if there are not many\"\n",
        "                    \"instances of this message)\"\n",
        "                )\n",
        "        self.car = Car(self.world, *self.track[0][1:4])\n",
        "\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "        return self.step(None)[0], {}\n",
        "\n",
        "    def step(self, action: Union[np.ndarray, int]):\n",
        "        assert self.car is not None\n",
        "\n",
        "        #############\n",
        "        # constant speed\n",
        "        if self.constant_speed != 0:\n",
        "            for w in self.car.wheels[0:4]:\n",
        "                    w.omega = self.constant_speed\n",
        "        #############\n",
        "\n",
        "        if action is not None:\n",
        "            if self.continuous:\n",
        "                self.car.steer(-action[0])\n",
        "                #self.car.gas(action[1])\n",
        "                #self.car.brake(action[2])\n",
        "            else:\n",
        "                if not self.action_space.contains(action):\n",
        "                    raise InvalidAction(\n",
        "                        f\"you passed the invalid action `{action}`. \"\n",
        "                        f\"The supported action_space is `{self.action_space}`\"\n",
        "                    )\n",
        "                self.car.steer(-0.6 * (action == 1) + 0.6 * (action == 2))\n",
        "                self.car.gas(0.2 * (action == 3))\n",
        "                self.car.brake(0.8 * (action == 4))\n",
        "\n",
        "        self.car.step(1.0 / FPS)\n",
        "        self.world.Step(1.0 / FPS, 6 * 30, 2 * 30)\n",
        "        self.t += 1.0 / FPS\n",
        "\n",
        "\n",
        "        # Updating state\n",
        "        #if variable speed is on, add speed as normalized state\n",
        "        if VARIABLE_SPEED[\"On\"]:\n",
        "            self.state = self.getState()\n",
        "        else:\n",
        "            self.state = self.getState()[0:2]\n",
        "        #print(self.state)\n",
        "        self.update_prev_errors(self.state[1])\n",
        "\n",
        "        step_reward = 0\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "\n",
        "        if action is not None:  # First step without action, called from reset()\n",
        "\n",
        "            # Penalize oscilations using CTE's variance\n",
        "            if(self.episode_steps>=NUM_PREV_ERRORS):\n",
        "                self.reward-= self.get_CTE_variance()*CTE_RESCALE\n",
        "\n",
        "            # Reward low CTE\n",
        "            if abs(self.state[1]) <= self.road_half_width:\n",
        "              self.reward += REWARD_VSHIFT - self.state[1]**2\n",
        "\n",
        "            self.car.fuel_spent = 0.0\n",
        "            step_reward = self.reward - self.prev_reward\n",
        "            self.placeholder = step_reward\n",
        "\n",
        "            self.prev_reward = self.reward\n",
        "\n",
        "            if self.tile_visited_count == len(self.track) or self.new_lap:\n",
        "                truncated = True\n",
        "\n",
        "            if abs(self.get_cross_track_error(self.car, self.track)[1]) > self.road_half_width:\n",
        "                step_reward = OFF_ROAD_PENALTY\n",
        "                terminated = True\n",
        "\n",
        "            # End episode when car goes off road\n",
        "            self.episode_steps+=1\n",
        "            if self.episode_steps > 2000 or self.new_lap:\n",
        "                terminated = True\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "        return self.state, step_reward, terminated, truncated, {}\n",
        "\n",
        "    def render(self):\n",
        "        if self.render_mode is None:\n",
        "            assert self.spec is not None\n",
        "            gym.logger.warn(\n",
        "                \"You are calling render method without specifying any render mode. \"\n",
        "                \"You can specify the render_mode at initialization, \"\n",
        "                f'e.g. gym.make(\"{self.spec.id}\", render_mode=\"rgb_array\")'\n",
        "            )\n",
        "            return\n",
        "        else:\n",
        "            return self._render(self.render_mode)\n",
        "\n",
        "    def _render(self, mode: str):\n",
        "        assert mode in self.metadata[\"render_modes\"]\n",
        "\n",
        "        pygame.font.init()\n",
        "        if self.screen is None and mode == \"human\":\n",
        "            pygame.init()\n",
        "            pygame.display.init()\n",
        "            self.screen = pygame.display.set_mode((WINDOW_W, WINDOW_H))\n",
        "        if self.clock is None:\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "        if \"t\" not in self.__dict__:\n",
        "            return  # reset() not called yet\n",
        "\n",
        "        self.surf = pygame.Surface((WINDOW_W, WINDOW_H))\n",
        "\n",
        "        assert self.car is not None\n",
        "        # computing transformations\n",
        "        angle = -self.car.hull.angle\n",
        "        # Animating first second zoom.\n",
        "        zoom = 0.1 * SCALE * max(1 - self.t, 0) + ZOOM * SCALE * min(self.t, 1)\n",
        "        scroll_x = -(self.car.hull.position[0]) * zoom\n",
        "        scroll_y = -(self.car.hull.position[1]) * zoom\n",
        "        trans = pygame.math.Vector2((scroll_x, scroll_y)).rotate_rad(angle)\n",
        "        trans = (WINDOW_W / 2 + trans[0], WINDOW_H / 4 + trans[1])\n",
        "\n",
        "        self._render_road(zoom, trans, angle)\n",
        "        self.car.draw(\n",
        "            self.surf,\n",
        "            zoom,\n",
        "            trans,\n",
        "            angle,\n",
        "            mode not in [\"state_pixels_list\", \"state_pixels\"],\n",
        "        )\n",
        "\n",
        "        self.surf = pygame.transform.flip(self.surf, False, True)\n",
        "\n",
        "        # showing stats\n",
        "        self._render_indicators(WINDOW_W, WINDOW_H)\n",
        "\n",
        "        font = pygame.font.Font(pygame.font.get_default_font(), 42)\n",
        "\n",
        "        ########################################\n",
        "\n",
        "\n",
        "        text = font.render(\"%.2f | %.2f\" %  (self.placeholder,  self.get_CTE_variance() * 200), True, (255, 255, 255), (0, 0, 0))\n",
        "\n",
        "\n",
        "        ########################################\n",
        "\n",
        "        text_rect = text.get_rect()\n",
        "        text_rect.center = (120, WINDOW_H - WINDOW_H * 2.5 / 40.0)\n",
        "        self.surf.blit(text, text_rect)\n",
        "\n",
        "        if mode == \"human\":\n",
        "            pygame.event.pump()\n",
        "            self.clock.tick(self.metadata[\"render_fps\"])\n",
        "            assert self.screen is not None\n",
        "            self.screen.fill(0)\n",
        "            self.screen.blit(self.surf, (0, 0))\n",
        "            pygame.display.flip()\n",
        "        elif mode == \"rgb_array\":\n",
        "            return self._create_image_array(self.surf, (VIDEO_W, VIDEO_H))\n",
        "        elif mode == \"state_pixels\":\n",
        "            return self._create_image_array(self.surf, (STATE_W, STATE_H))\n",
        "        else:\n",
        "            return self.isopen\n",
        "\n",
        "    def _render_road(self, zoom, translation, angle):\n",
        "        bounds = PLAYFIELD\n",
        "        field = [\n",
        "            (bounds, bounds),\n",
        "            (bounds, -bounds),\n",
        "            (-bounds, -bounds),\n",
        "            (-bounds, bounds),\n",
        "        ]\n",
        "\n",
        "        # draw background\n",
        "        self._draw_colored_polygon(\n",
        "            self.surf, field, self.bg_color, zoom, translation, angle, clip=False\n",
        "        )\n",
        "\n",
        "        # draw grass patches\n",
        "        grass = []\n",
        "        for x in range(-20, 20, 2):\n",
        "            for y in range(-20, 20, 2):\n",
        "                grass.append(\n",
        "                    [\n",
        "                        (GRASS_DIM * x + GRASS_DIM, GRASS_DIM * y + 0),\n",
        "                        (GRASS_DIM * x + 0, GRASS_DIM * y + 0),\n",
        "                        (GRASS_DIM * x + 0, GRASS_DIM * y + GRASS_DIM),\n",
        "                        (GRASS_DIM * x + GRASS_DIM, GRASS_DIM * y + GRASS_DIM),\n",
        "                    ]\n",
        "                )\n",
        "        for poly in grass:\n",
        "            self._draw_colored_polygon(\n",
        "                self.surf, poly, self.grass_color, zoom, translation, angle\n",
        "            )\n",
        "\n",
        "        # draw road\n",
        "        i= 0\n",
        "        for poly, color in self.road_poly:\n",
        "            # converting to pixel coordinates\n",
        "            poly = [(p[0], p[1]) for p in poly]\n",
        "            color = [int(c) for c in color]\n",
        "            self._draw_colored_polygon(self.surf, poly, color, zoom, translation, angle)\n",
        "\n",
        "            i+=1\n",
        "\n",
        "\n",
        "    def _render_indicators(self, W, H):\n",
        "        s = W / 40.0\n",
        "        h = H / 40.0\n",
        "        color = (0, 0, 0)\n",
        "        polygon = [(W, H), (W, H - 5 * h), (0, H - 5 * h), (0, H)]\n",
        "        pygame.draw.polygon(self.surf, color=color, points=polygon)\n",
        "\n",
        "        def vertical_ind(place, val):\n",
        "            return [\n",
        "                (place * s, H - (h + h * val)),\n",
        "                ((place + 1) * s, H - (h + h * val)),\n",
        "                ((place + 1) * s, H - h),\n",
        "                ((place + 0) * s, H - h),\n",
        "            ]\n",
        "\n",
        "        def horiz_ind(place, val):\n",
        "            return [\n",
        "                ((place + 0) * s, H - 4 * h),\n",
        "                ((place + val) * s, H - 4 * h),\n",
        "                ((place + val) * s, H - 2 * h),\n",
        "                ((place + 0) * s, H - 2 * h),\n",
        "            ]\n",
        "\n",
        "        assert self.car is not None\n",
        "        true_speed = np.sqrt(\n",
        "            np.square(self.car.hull.linearVelocity[0])\n",
        "            + np.square(self.car.hull.linearVelocity[1])\n",
        "        )\n",
        "\n",
        "        # simple wrapper to render if the indicator value is above a threshold\n",
        "        def render_if_min(value, points, color):\n",
        "            if abs(value) > 1e-4:\n",
        "                pygame.draw.polygon(self.surf, points=points, color=color)\n",
        "\n",
        "        render_if_min(true_speed, vertical_ind(5, 0.02 * true_speed), (255, 255, 255))\n",
        "        # ABS sensors\n",
        "        render_if_min(\n",
        "            self.car.wheels[0].omega,\n",
        "            vertical_ind(7, 0.01 * self.car.wheels[0].omega),\n",
        "            (0, 0, 255),\n",
        "        )\n",
        "        render_if_min(\n",
        "            self.car.wheels[1].omega,\n",
        "            vertical_ind(8, 0.01 * self.car.wheels[1].omega),\n",
        "            (0, 0, 255),\n",
        "        )\n",
        "        render_if_min(\n",
        "            self.car.wheels[2].omega,\n",
        "            vertical_ind(9, 0.01 * self.car.wheels[2].omega),\n",
        "            (51, 0, 255),\n",
        "        )\n",
        "        render_if_min(\n",
        "            self.car.wheels[3].omega,\n",
        "            vertical_ind(10, 0.01 * self.car.wheels[3].omega),\n",
        "            (51, 0, 255),\n",
        "        )\n",
        "\n",
        "        render_if_min(\n",
        "            self.car.wheels[0].joint.angle,\n",
        "            horiz_ind(20, -10.0 * self.car.wheels[0].joint.angle),\n",
        "            (0, 255, 0),\n",
        "        )\n",
        "        render_if_min(\n",
        "            self.car.hull.angularVelocity,\n",
        "            horiz_ind(30, -0.8 * self.car.hull.angularVelocity),\n",
        "            (255, 0, 0),\n",
        "        )\n",
        "\n",
        "    def _draw_colored_polygon(\n",
        "        self, surface, poly, color, zoom, translation, angle, clip=True\n",
        "    ):\n",
        "        poly = [pygame.math.Vector2(c).rotate_rad(angle) for c in poly]\n",
        "        poly = [\n",
        "            (c[0] * zoom + translation[0], c[1] * zoom + translation[1]) for c in poly\n",
        "        ]\n",
        "        # This checks if the polygon is out of bounds of the screen, and we skip drawing if so.\n",
        "        # Instead of calculating exactly if the polygon and screen overlap,\n",
        "        # we simply check if the polygon is in a larger bounding box whose dimension\n",
        "        # is greater than the screen by MAX_SHAPE_DIM, which is the maximum\n",
        "        # diagonal length of an environment object\n",
        "        if not clip or any(\n",
        "            (-MAX_SHAPE_DIM <= coord[0] <= WINDOW_W + MAX_SHAPE_DIM)\n",
        "            and (-MAX_SHAPE_DIM <= coord[1] <= WINDOW_H + MAX_SHAPE_DIM)\n",
        "            for coord in poly\n",
        "        ):\n",
        "            gfxdraw.aapolygon(self.surf, poly, color)\n",
        "            gfxdraw.filled_polygon(self.surf, poly, color)\n",
        "\n",
        "    def _create_image_array(self, screen, size):\n",
        "        scaled_screen = pygame.transform.smoothscale(screen, size)\n",
        "        return np.transpose(\n",
        "            np.array(pygame.surfarray.pixels3d(scaled_screen)), axes=(1, 0, 2)\n",
        "        )\n",
        "\n",
        "    def close(self):\n",
        "        if self.screen is not None:\n",
        "            pygame.display.quit()\n",
        "            self.isopen = False\n",
        "            pygame.quit()\n",
        "\n",
        "\n",
        "\n",
        "    ##########################################################\n",
        "\n",
        "    ##########################################################\n",
        "\n",
        "    # Added methods\n",
        "\n",
        "    def point_segment_dist(self, p, a, b):\n",
        "        n = b - a\n",
        "        norm_n = np.linalg.norm(n)\n",
        "        if norm_n < 1e-10:\n",
        "            return np.linalg.norm(p - a)\n",
        "\n",
        "        n = n / norm_n\n",
        "        ap = a - p\n",
        "        proj_on_line = ap.dot(n) * n\n",
        "\n",
        "        if np.linalg.norm(proj_on_line) > norm_n:\n",
        "            return min(np.linalg.norm(p - a), np.linalg.norm(p - b))\n",
        "\n",
        "        return np.linalg.norm(ap - proj_on_line)\n",
        "\n",
        "    def get_cross_track_error(self, car, track):\n",
        "        # steer in [-1, 1], gas in [0, 1], break in [0 ,1]\n",
        "        pld_min = np.finfo(float).max\n",
        "        dest_min = 0\n",
        "\n",
        "        p = car.hull.position\n",
        "        p = np.array([p[0], p[1]])\n",
        "\n",
        "        for i in range(1, len(track)):\n",
        "            ai = np.array([track[i-1][2], track[i-1][3]])\n",
        "            bi = np.array([track[i][2], track[i][3]])\n",
        "            pld = self.point_segment_dist(p, ai, bi)\n",
        "            if pld < pld_min:\n",
        "                pld_min = pld\n",
        "                dest_min = i\n",
        "\n",
        "        target_heading = track[dest_min][1]\n",
        "        error_heading = target_heading - car.hull.angle\n",
        "        error_heading =  atan2(sin(error_heading), cos(error_heading))\n",
        "\n",
        "        R_world_trackframe = np.array([ [cos(target_heading), sin(target_heading)],\n",
        "                                        [-sin(target_heading), cos(target_heading)] ])\n",
        "\n",
        "        p_trackframe_world = np.array( track[dest_min][2:4] ).reshape((2,1))\n",
        "        p_car_world = np.array( [car.hull.position[0], car.hull.position[1]] ).reshape((2,1))\n",
        "\n",
        "        p_car_trackframe = R_world_trackframe.dot(p_car_world - p_trackframe_world)\n",
        "        error_dist = -1 * p_car_trackframe[0][0]\n",
        "\n",
        "        #print (error_heading * 180.0 / 3.14, error_dist, p_car_trackframe[1][0])\n",
        "        return error_heading, error_dist, dest_min\n",
        "\n",
        "    def get_CTE_variance(self):\n",
        "        # Calculate the mean\n",
        "        mean = sum(self.prev_errors) / len(self.prev_errors)\n",
        "\n",
        "        # Calculate the squared differences from the mean\n",
        "        squared_diff = [(x - mean) ** 2 for x in self.prev_errors]\n",
        "\n",
        "        # Calculate the variance\n",
        "        return sum(squared_diff) / len(self.prev_errors)\n",
        "\n",
        "    def getState(self):\n",
        "\n",
        "        CTE = self.get_cross_track_error(self.car, self.track)[0:2]\n",
        "\n",
        "        normalized_error_heading = 2 * CTE[0] / math.pi\n",
        "\n",
        "        normalized_CTE = CTE[1] / self.road_half_width\n",
        "\n",
        "        normalized_speed = ((self.constant_speed - VARIABLE_SPEED[\"min_speed\"]) / (\n",
        "            VARIABLE_SPEED[\"max_speed\"]-VARIABLE_SPEED[\"min_speed\"])) * (0.99) + 0.01\n",
        "\n",
        "        return np.array([normalized_error_heading, normalized_CTE, normalized_speed], dtype=np.float64)\n",
        "\n",
        "    def update_prev_errors(self, cur_error):\n",
        "        self.prev_errors.insert(0, cur_error)\n",
        "        self.prev_errors.pop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqp0Ypf2XyXs",
        "outputId": "8f4ae28e-37ab-4232-88da-93f5eb011807"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Register Environment"
      ],
      "metadata": {
        "id": "zKtVK9f0ber8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Registering custom enviroment\n",
        "\n",
        "def registerEnv(ID):\n",
        "    gym.envs.register(\n",
        "        id=ID,\n",
        "        entry_point=lambda:CarRacing,  # Specify the module and class name\n",
        "        max_episode_steps=2000,\n",
        "        kwargs={}\n",
        "    )\n",
        "\n",
        "env_id  = 'center_aligning'\n",
        "# Register environment\n",
        "registerEnv(env_id)"
      ],
      "metadata": {
        "id": "Y5oqF13lbjAv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Policy: TD3"
      ],
      "metadata": {
        "id": "24XRXrhKYfb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actor network"
      ],
      "metadata": {
        "id": "eysonnWaY0oG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action, l1_dim=100, l2_dim=100):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.l1 = nn.Linear(state_dim, l1_dim)\n",
        "        self.l2 = nn.Linear(l1_dim, l2_dim)\n",
        "        self.l3 = nn.Linear(l2_dim, action_dim)\n",
        "\n",
        "        self.max_action = max_action\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = relu(self.l1(x))\n",
        "        x = relu(self.l2(x))\n",
        "        x = self.max_action * self.tanh(self.l3(x))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "d7nqJG37YCfx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Critic Netwrok"
      ],
      "metadata": {
        "id": "MUR8XcygZJa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Q_function(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, l1_dim=100, l2_dim=100):\n",
        "        super(Q_function, self).__init__()\n",
        "        self.l1 = nn.Linear(state_dim + action_dim, l1_dim)\n",
        "        self.l2 = nn.Linear(l1_dim, l2_dim)\n",
        "        self.l3 = nn.Linear(l2_dim, 1)\n",
        "\n",
        "    def forward(self, x, a):\n",
        "        x = torch.cat([x, a], 1)\n",
        "\n",
        "        x = relu(self.l1(x))\n",
        "        x = relu(self.l2(x))\n",
        "        x = self.l3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, q1_l1_dim=100, q1_l2_dim=100,\n",
        "                 q2_l1_dim=100, q2_l2_dim=100):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        # Q1\n",
        "        self.q1 = Q_function(state_dim, action_dim, q1_l1_dim, q1_l2_dim)\n",
        "        # Q2\n",
        "        self.q2 = Q_function(state_dim, action_dim, q2_l1_dim, q2_l2_dim)\n",
        "\n",
        "    def forward(self, x, a):\n",
        "        x1 = self.q1(x, a)\n",
        "        x2 = self.q2(x, a)\n",
        "\n",
        "        return x1, x2"
      ],
      "metadata": {
        "id": "VjhLlRMeZNHu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Twin Delayed Deep Deterministic Policy (TD3)"
      ],
      "metadata": {
        "id": "EFqHAtHWZap0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TD3(object):\n",
        "    def __init__(\n",
        "\t\tself,\n",
        "\t\tstate_dim,\n",
        "\t\taction_dim,\n",
        "\t\tmax_action,\n",
        "\t\tdiscount=0.99,\n",
        "\t\tpolicy_noise=0.2,\n",
        "\t\tnoise_clip=0.5,\n",
        "\t\tpolicy_freq=2\n",
        "\t    ):\n",
        "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target = copy.deepcopy(self.actor)\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
        "\n",
        "        self.critic = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_target = copy.deepcopy(self.critic)\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
        "\n",
        "        self.max_action = max_action\n",
        "        self.discount = discount\n",
        "        self.policy_noise = policy_noise\n",
        "        self.noise_clip = noise_clip\n",
        "        self.policy_freq = policy_freq\n",
        "        self.total_it = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "        return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "    def select_vectorized_action(self, states):\n",
        "        states_tensor = torch.FloatTensor(states).to(device)\n",
        "        actions = self.actor(states_tensor)\n",
        "        return actions.cpu().data.numpy()\n",
        "\n",
        "    def train(self, iterations, replay_buffer, tau, batch_size=256):\n",
        "        for i in range(iterations):\n",
        "            self.total_it += 1\n",
        "\n",
        "            s, ns, ac, r, d = replay_buffer.sample(batch_size)\n",
        "\n",
        "            state = torch.FloatTensor(s).to(device)\n",
        "            next_state = torch.FloatTensor(ns).to(device)\n",
        "            action = torch.FloatTensor(ac).to(device)\n",
        "            reward = torch.FloatTensor(r).to(device)\n",
        "            done = torch.FloatTensor(1 - d).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "\t\t\t    # For next action,  consider the policy and add noise\n",
        "                noise = (\n",
        "                    torch.randn_like(action) * self.policy_noise\n",
        "                    ).clamp(-self.noise_clip, self.noise_clip)\n",
        "\n",
        "                next_action = (\n",
        "\t\t\t\t    self.actor_target(next_state) + noise\n",
        "\t\t\t    ).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "\t\t\t    # Compute the target Q value\n",
        "                target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "                min_target = torch.min(target_Q1, target_Q2)\n",
        "                target_Q = reward + (done * self.discount * min_target)\n",
        "\n",
        "            # Get current Q estimates\n",
        "            curr_Q1, curr_Q2 = self.critic(state, action)\n",
        "\n",
        "            # Calculate loss for crtic\n",
        "            mse_loss_1 = nn.MSELoss()\n",
        "            mse_loss_2 = nn.MSELoss()\n",
        "            loss_for_critic =  mse_loss_1(curr_Q1, target_Q) + mse_loss_2(curr_Q2, target_Q)\n",
        "\n",
        "            # Optimization for the critic\n",
        "            self.critic_optimizer.zero_grad()\n",
        "            loss_for_critic.backward()\n",
        "            self.critic_optimizer.step()\n",
        "\n",
        "            # update the policy\n",
        "            if i % self.policy_freq == 0:\n",
        "                loss_for_actor = -self.critic.q2(state, self.actor(state)).mean()\n",
        "\n",
        "                # Optimize the actor\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                loss_for_actor.backward()\n",
        "                self.actor_optimizer.step()\n",
        "\n",
        "                # Update the frozen target models\n",
        "                for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "    def save(self, filename, directory):\n",
        "        torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "        torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "\n",
        "    def load(self, filename, directory):\n",
        "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename), map_location=torch.device('cpu')))\n",
        "        self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename), map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "id": "48mV0XWdZjNa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training"
      ],
      "metadata": {
        "id": "3opGv_5rZu6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Utilities"
      ],
      "metadata": {
        "id": "OGA2TL8KabYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, max_size=1e4):\n",
        "        self.storage = []\n",
        "        self.max_size = max_size\n",
        "        self.ind = 0\n",
        "\n",
        "    def add(self, state, next_state, action, reward, done):\n",
        "        data = [state, next_state, action, reward, done]\n",
        "\n",
        "        # if there is still space in storage, add data\n",
        "        if len(self.storage) < self.max_size:\n",
        "            self.storage.append(data)\n",
        "            # space met, reset index back to 0\n",
        "        else:\n",
        "            self.storage[self.ind] = data\n",
        "            self.ind += 1\n",
        "            if self.ind == self.max_size:\n",
        "                self.ind = 0\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # randomly sample batch size number of past events\n",
        "        indices = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "        states, next_states, actions, rewards, done = [], [], [], [], []\n",
        "\n",
        "        for i in indices:\n",
        "\n",
        "            s, ns, ac, r, d = self.storage[i]\n",
        "            states.append(np.array(s, copy=False))\n",
        "            next_states.append(np.array(ns, copy=False))\n",
        "            actions.append(np.array(ac, copy=False))\n",
        "            rewards.append(np.array(r, copy=False))\n",
        "            done.append(np.array(d, copy=False))\n",
        "\n",
        "        return np.array(states), np.array(next_states), np.array(actions), \\\n",
        "            np.array(rewards).reshape(-1, 1), np.array(done).reshape(-1, 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "8BynWslWafOv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop"
      ],
      "metadata": {
        "id": "S7vRsQyNaWAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "MAX_TIME_STEPS = 1000000\n",
        "max_episode_steps = 2000\n",
        "\n",
        "NUM_PARALLEL_ENVS = 3\n",
        "FIN_EPISODES_BEFORE_TRAIN = 4\n",
        "\n",
        "#Options to change expl noise and tau\n",
        "LOWER_EXPL_NOISE = {\"On\" : True, \"Reward_Threshold\":14000, 'Value': 0.001}\n",
        "LOWER_TAU = {\"On\" : True, \"Reward_Threshold\":18000, 'Value': 0.0005}\n",
        "\n",
        "#load already trained policy\n",
        "LOAD_POLICY = {\"On\": False, 'init_time_steps': 1e4}\n",
        "\n",
        "#Avg reward termination condition\n",
        "AVG_REWARD_TERMIN_THRESHOLD = 19000\n",
        "# Time steps below which a standard training iteration param is passed\n",
        "MIN_EPS_TIMESTEPS = 500\n",
        "\n",
        "# Specify the file name\n",
        "LOGS_FILEPATH = './benchmarks/logs/TD3_log.csv'\n",
        "\n",
        "with open(LOGS_FILEPATH, 'w', newline='') as file:\n",
        "\tlog_writer = csv.writer(file)\n",
        "\n",
        "\t# Write headings\n",
        "\tlog_writer.writerow(['r', 'l'])\n",
        "\n",
        "# Runs policy for X episodes and returns average reward\n",
        "def evaluate_policy(policy, eval_episodes=5):\n",
        "\tavg_reward = 0\n",
        "\tnum_fin_episodes = 0\n",
        "\tobs, info = envs.reset()\n",
        "\tavg = 0\n",
        "\twhile num_fin_episodes < eval_episodes:\n",
        "\t\taction = policy.select_vectorized_action(np.array(obs))\n",
        "\t\tobs, reward, done, _, info = envs.step(action)\n",
        "\t\tavg_reward += reward\n",
        "\n",
        "        # when an episode ends in any environment\n",
        "\t\tif info.keys():\n",
        "\n",
        "\t\t\tfinished = info['_final_observation']\n",
        "\t\t\tnum_fin = np.count_nonzero(finished)\n",
        "\n",
        "\t\t\tnum_fin_episodes += num_fin\n",
        "\n",
        "\t\t\tavg += np.sum(avg_reward[finished])\n",
        "\n",
        "\tavg /= num_fin_episodes\n",
        "\tprint(\"---------------------------------------\")\n",
        "\tprint(\"Evaluation over %d episodes: %f\" % (num_fin_episodes, avg))\n",
        "\tprint(\"---------------------------------------\")\n",
        "\treturn avg\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "\tstart_timesteps = 1e3           \t# How many time steps purely random policy is run for\n",
        "\teval_freq = 1e4\t\t\t             # How often (time steps) we evaluate\n",
        "\tmax_timesteps = MAX_TIME_STEPS \t\t# Max time steps to run environment for\n",
        "\tsave_models = True\t\t\t    \t# Whether or not models are saved\n",
        "\n",
        "\texpl_noise=0.01\t\t                # Std of Gaussian exploration noise\n",
        "\tbatch_size=256\t\t                # Batch size for both actor and critic\n",
        "\ttau=0.005\t\t                    # Target network update rate\n",
        "\tpolicy_noise=0.1\t\t              # Noise added to target policy during critic update\n",
        "\tnoise_clip=0.25\t                  # Range to clip target policy noise\n",
        "\n",
        "\n",
        "\tfile_name = \"TD3_%s\" % ( str(SEED))\n",
        "\tprint(\"---------------------------------------\")\n",
        "\tprint (\"Settings: %s\" % (file_name))\n",
        "\tprint(\"---------------------------------------\")\n",
        "\n",
        "\tif not os.path.exists(\"./results\"):\n",
        "\t\tos.makedirs(\"./results\")\n",
        "\tif save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "\t\tos.makedirs(\"./pytorch_models\")\n",
        "\n",
        "\t# Register environment\n",
        "\tenv_id  = 'center_maintaining'\n",
        "\tenv.registerEnv(env_id)\n",
        "\n",
        "\t# Initialise vectorized environment\n",
        "\tnum_envs= NUM_PARALLEL_ENVS\n",
        "\tenvs = gym.make_vec(env_id, num_envs=num_envs, render_mode='human')\n",
        "\n",
        "\t#Counter to track finished episode within one iteration of parallel runs\n",
        "\tnum_fin_episodes = 0\n",
        "\n",
        "\t# Set seeds\n",
        "\ttorch.manual_seed(SEED)\n",
        "\tnp.random.seed(SEED)\n",
        "\n",
        "\tstate_dim = envs.single_observation_space.shape[0]\n",
        "\taction_dim = envs.single_action_space.shape[0]\n",
        "\tmax_action = float(envs.single_action_space.high[0])\n",
        "\n",
        "\t# Initialize policy\n",
        "\tpolicy = TD3(state_dim, action_dim, max_action, policy_noise=policy_noise, noise_clip=noise_clip)\n",
        "\n",
        "\t# Load already trained policy\n",
        "\tif LOAD_POLICY[\"On\"]:\n",
        "\t\tfilename = \"Policy_19(1)\"\n",
        "\t\tdirectory = \"./policies\"\n",
        "\t\tpolicy.load(filename, directory)\n",
        "\t\tstart_timesteps = 0\n",
        "\n",
        "\t# Init replay buffer\n",
        "\treplay_buffer = ReplayBuffer()\n",
        "\n",
        "\t# Evaluate untrained policy\n",
        "\tevaluations = []#evaluations = [evaluate_policy(policy)]\n",
        "\n",
        "\ttotal_timesteps = 0\n",
        "\ttimesteps_since_eval = 0\n",
        "\ttrain_iteration = 0\n",
        "\n",
        "\t# array to track if the frist episode in all parallel env have ended\n",
        "\tall_done = np.full(num_envs, True, dtype=bool)\n",
        "\n",
        "\tepisode_count = 0\n",
        "\tavg_reward = 0\n",
        "\n",
        "\tt0 = time.time()\n",
        "\n",
        "\twhile total_timesteps < max_timesteps:\n",
        "\n",
        "\t\tif all_done.all():\n",
        "\n",
        "\t\t\t# calculate average reward over episodes\n",
        "\t\t\tif num_fin_episodes!=0: avg_reward /= num_fin_episodes\n",
        "\n",
        "\t\t\tif total_timesteps != 0 and (not LOAD_POLICY['On'] or total_timesteps>=LOAD_POLICY[\"init_time_steps\"]):\n",
        "\n",
        "\t\t\t\tprint(\"\\nData Stats:\\nTotal T: %d   Train itr: %d   Episodes T: %d   Best Reward: %f   Avg Reward: %f   --  Wallclk T: %d sec\" % \\\n",
        "\t\t\t\t\t(total_timesteps, train_iteration, episode_timesteps, max_reward, avg_reward, int(time.time() - t0)))\n",
        "\n",
        "\t\t\t\t# Store metrics\n",
        "\t\t\t\twith open(LOGS_FILEPATH, 'a', newline='') as file:\n",
        "\t\t\t\t\tlog_writer = csv.writer(file)\n",
        "\t\t\t\t\tlog_writer.writerow([avg_reward, episode_timesteps/num_fin_episodes])\n",
        "\n",
        "\t\t\t\tif avg_reward >= AVG_REWARD_TERMIN_THRESHOLD:\n",
        "\t\t\t\t\tprint(\"\\n\\nAvg Reward Threshold Met -- Training Terminated\\n\")\n",
        "\t\t\t\t\tbreak\n",
        "\n",
        "\t\t\t\t# Lower learning rate\n",
        "\t\t\t\tif LOWER_TAU[\"On\"] and avg_reward >= LOWER_TAU[\"Reward_Threshold\"]:\n",
        "\t\t\t\t\tprint(\"\\n-------Lowered Tau to %f \\n\" % LOWER_TAU[\"Value\"])\n",
        "\t\t\t\t\tLOWER_TAU[\"On\"] = False\n",
        "\n",
        "                # Lower exploration noise\n",
        "\t\t\t\tif LOWER_EXPL_NOISE[\"On\"] and avg_reward >= LOWER_EXPL_NOISE[\"Reward_Threshold\"]:\n",
        "\t\t\t\t\texpl_noise = expl_noise / 2\n",
        "\t\t\t\t\tprint(\"\\n-------Lowered expl noise to %f \\n\" % LOWER_EXPL_NOISE[\"Value\"])\n",
        "\t\t\t\t\tLOWER_EXPL_NOISE[\"On\"] = False\n",
        "\n",
        "\t\t\t\t# save each policy with above stats before training\n",
        "\t\t\t\tpolicy.save(\"Policy_%d\" % (train_iteration), directory=\"./policies\")\n",
        "\n",
        "\t\t\t\tprint(\"\\nTraining: \", end=\" \")\n",
        "\t\t\t\tif episode_timesteps < MIN_EPS_TIMESTEPS:\n",
        "\t\t\t\t\tprint(\"STANDARDIZED TRAINING ITERATIONS\")\n",
        "\t\t\t\t\tpolicy.train(MIN_EPS_TIMESTEPS, replay_buffer, tau, batch_size)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tpolicy.train(episode_timesteps, replay_buffer, tau, batch_size)\n",
        "\n",
        "\t\t\t\tprint(\"-Finished \")\n",
        "\t\t\t\tprint(\"\\n-----------------------\")\n",
        "\n",
        "\t\t\t# Evaluate episode\n",
        "\t\t\tif timesteps_since_eval >= eval_freq:\n",
        "\t\t\t\ttimesteps_since_eval %= eval_freq\n",
        "\t\t\t\teval_score = evaluate_policy(policy)\n",
        "\t\t\t\tevaluations.append(eval_score)\n",
        "\n",
        "\t\t\t\tif save_models: policy.save(file_name, directory=\"./pytorch_models\")\n",
        "\t\t\t\tnp.save(\"./results/%s\" % (file_name), evaluations)\n",
        "\n",
        "\t\t\t# Reset environment\n",
        "\t\t\tprint(\"\\nCollecting data:\")\n",
        "\n",
        "\t\t\tobs, info = envs.reset(seed=[SEED + i for i in range(num_envs)])\n",
        "\t\t\tSEED+=num_envs\n",
        "\n",
        "\t\t\tall_done = np.full(num_envs, False, dtype=bool)\n",
        "\t\t\tepisode_reward = np.zeros(num_envs, dtype=float)\n",
        "\t\t\tepisode_timesteps = 0\n",
        "\t\t\ttrain_iteration += 1\n",
        "\n",
        "\t\t\tmax_reward = None\n",
        "\t\t\tavg_reward = 0\n",
        "\t\t\tnum_fin_episodes = 0\n",
        "\n",
        "\t\t# Select action randomly or according to policy\n",
        "\t\tif total_timesteps == start_timesteps:\n",
        "\t\t\tprint(\"\\n\\n\\nPolicy actions started\\n\\n\\n\")\n",
        "\n",
        "\t\tif total_timesteps < start_timesteps:\n",
        "\t\t\t# Random actions for each environment\n",
        "\t\t\taction = envs.action_space.sample()\n",
        "\t\telse:\n",
        "\t\t\taction = policy.select_vectorized_action(obs)\n",
        "\n",
        "\t\t\tif expl_noise != 0:\n",
        "\t\t\t\taction = (action + np.random.normal(0, expl_noise, size=envs.single_action_space.shape[0])).clip(envs.single_action_space.low, envs.single_action_space.high)\n",
        "\n",
        "\t\t# Perform action\n",
        "\t\tnew_obs, reward, done, truncated, info = envs.step(action)\n",
        "\t\tepisode_reward += reward\n",
        "\n",
        "\t\t# when an episode ends in any environment\n",
        "\t\tif info.keys():\n",
        "\n",
        "\t\t\tfinished = info['_final_observation']\n",
        "\t\t\tnum_fin = np.count_nonzero(finished)\n",
        "\n",
        "\t\t\tnum_fin_episodes += num_fin\n",
        "\t\t\tepisode_count += num_fin\n",
        "\n",
        "            # all_done marks the environments whose episodes ended\n",
        "\t\t\tall_done = np.logical_or(all_done, finished)\n",
        "\n",
        "\t\t\tprint(\"Episode%d reward for finished enviroments:\" % episode_count, episode_reward[finished])\n",
        "\n",
        "            #Set min reward among finished episodes\n",
        "\t\t\tif max_reward is not None:\n",
        "\t\t\t\tmax_reward = max(max_reward, max(episode_reward[finished]))\n",
        "\t\t\telse:\n",
        "\t\t\t\tmax_reward = max(episode_reward[finished])\n",
        "\n",
        "\t\t\tavg_reward += sum(episode_reward[finished])\n",
        "\n",
        "\t\t\t#set episode reward for respective environments 0\n",
        "\t\t\tepisode_reward[finished] = 0\n",
        "\n",
        "\t\tdone_bool = np.full(num_envs, False, dtype=bool) if episode_timesteps + 1 == max_episode_steps else all_done\n",
        "\n",
        "\t\t# Store data in replay buffer\n",
        "\t\tfor i in range(num_envs):\n",
        "\t\t\tif info.keys() and info['_final_observation'][i] == True:\n",
        "\t\t\t\treplay_buffer.add(obs[i], info['final_observation'][i], action[i], reward[i], float(all_done[i]))\n",
        "\t\t\telse:\n",
        "\t\t\t\treplay_buffer.add(obs[i], new_obs[i], action[i], reward[i], float(all_done[i]))\n",
        "\n",
        "\t\tobs = new_obs\n",
        "\n",
        "        #   Episode time_steps for all episodes in each environment\n",
        "\t\tepisode_timesteps += num_envs\n",
        "\t\ttotal_timesteps += num_envs\n",
        "\t\ttimesteps_since_eval += num_envs\n",
        "\n",
        "\t# Final evaluation\n",
        "\tevaluations.append(evaluate_policy(policy))\n",
        "\tif save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "\tnp.save(\"./results/%s\" % (file_name), evaluations)\n",
        "\n",
        "\tenvs.close()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "j__K0gdsZzuH",
        "outputId": "5f7dab3a-b95c-4460-fe34-25eef73f6516"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './benchmarks/logs/TD3_log.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-7ef23c2ef025>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mLOGS_FILEPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./benchmarks/logs/TD3_log.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOGS_FILEPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mlog_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './benchmarks/logs/TD3_log.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmarks"
      ],
      "metadata": {
        "id": "BMym-roPaun_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "PPO_TRAIN_TIME_STEPS = 2000\n",
        "DDPG_TRAIN_TIME_STEPS = 10000\n",
        "SAC_TRAIN_TIME_STEPS = 2000\n",
        "\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "def evaluate_policy(policy, env, num_episodes=1):\n",
        "    total_reward = 0\n",
        "    for i in range(num_episodes):\n",
        "        state, info = env.reset()\n",
        "        next_state = state\n",
        "        terminated, truncated = False, False\n",
        "        while not terminated and not truncated:\n",
        "            action = policy.predict(np.array(next_state), deterministic=True)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action[0])\n",
        "            total_reward += reward\n",
        "\n",
        "    avg_reward = total_reward / num_episodes\n",
        "\n",
        "    return avg_reward\n",
        "\n",
        "# logging directories for each algorithm\n",
        "\n",
        "#Monitor\n",
        "ppo_log_dir = \"./benchmarks/logs/ppo_logs/\"\n",
        "sac_log_dir = \"./benchmarks/logs/sac_logs/\"\n",
        "ddpg_log_dir = \"./benchmarks/logs/ddpg_logs/\"\n",
        "\n",
        "\n",
        "######## Training ########\n",
        "train = 1\n",
        "if train:\n",
        "\n",
        "    # set up loggers\n",
        "    ppo_logger = configure(ppo_log_dir, [\"stdout\", \"csv\"])\n",
        "    sac_logger = configure(sac_log_dir, [\"stdout\", \"csv\"])\n",
        "    ddpg_logger = configure(sac_log_dir, [\"stdout\", \"csv\"])\n",
        "\n",
        "    # Instantiate the env\n",
        "    ppo_env = CarRacing(render_mode = 'human')\n",
        "    sac_env = CarRacing(render_mode = 'human')\n",
        "    ddpg_env = CarRacing(render_mode = 'human')\n",
        "\n",
        "\n",
        "    # Create monitor wrappers for each algorithm with unique logging directories\n",
        "    ppo_env = Monitor(ppo_env, ppo_log_dir)\n",
        "    sac_env = Monitor(sac_env, sac_log_dir)\n",
        "    ddpg_env = Monitor(ddpg_env, ddpg_log_dir)\n",
        "\n",
        "    # PPO model\n",
        "    PPO_model = PPO(\"MlpPolicy\", ppo_env, verbose = 1, learning_rate= LEARNING_RATE)\n",
        "    PPO_model.set_logger(ppo_logger)\n",
        "\n",
        "    print(\"Training PPO\")\n",
        "    PPO_model.learn(total_timesteps=PPO_TRAIN_TIME_STEPS)\n",
        "    PPO_model.save(\"./benchmarks/ppo_policy\")\n",
        "\n",
        "    # SAC\n",
        "    SAC_model = SAC(\"MlpPolicy\", sac_env, verbose=1, learning_rate=LEARNING_RATE)\n",
        "    SAC_model.set_logger(sac_logger)\n",
        "\n",
        "    print(\"Training SAC\")\n",
        "    SAC_model.learn(total_timesteps=SAC_TRAIN_TIME_STEPS)\n",
        "    SAC_model.save(\"./benchmarks/sac_policy\")\n",
        "\n",
        "    # DDPG\n",
        "    # The noise objects for DDPG\n",
        "    n_actions = ddpg_env.action_space.shape[-1]\n",
        "    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.01 * np.ones(n_actions))\n",
        "\n",
        "    DDPG_model = DDPG(\"MlpPolicy\", ddpg_env, action_noise=action_noise, verbose=1, tau=0.001)\n",
        "    DDPG_model.set_logger(ddpg_logger)\n",
        "\n",
        "    print(\"Training DDPG\")\n",
        "    DDPG_model.learn(total_timesteps=DDPG_TRAIN_TIME_STEPS)\n",
        "    DDPG_model.save(\"./benchmarks/ddpg_policy\")\n",
        "\n",
        "    print(\"Training complete\")\n",
        "\n",
        "    ppo_env.close()\n",
        "    sac_env.close()\n",
        "    ddpg_env.close()\n",
        "\n",
        "\n",
        "\n",
        "######## Evaluation ########\n",
        "env = CarRacing(render_mode = 'human')\n",
        "\n",
        "PPO_model = PPO.load(\"./benchmarks/ppo_policy\")\n",
        "SAC_model = SAC.load(\"./benchmarks/sac_policy\")\n",
        "DDPG_model = DDPG.load(\"./benchmarks/ddpg_policy\")\n",
        "\n",
        "print(\"Evaluating PPO\")\n",
        "#PPO_avg_reward = evaluate_policy(PPO_model, env)\n",
        "\n",
        "print(\"Evaluating SAC\")\n",
        "SAC_avg_reward = evaluate_policy(SAC_model, env)\n",
        "\n",
        "print(\"Evaluating DDPG\")\n",
        "#DDPG_avg_reward = evaluate_policy(DDPG_model, env)\n",
        "\n",
        "env.close()\n",
        "\n",
        "#print(\"\\nPPO Average Reward:\", PPO_avg_reward)\n",
        "#print(\"\\nSAC Average Reward:\", SAC_avg_reward)\n",
        "#print(\"\\nDDPG Average Reward:\", DDPG_avg_reward)\n",
        "\n",
        "# Load monitoring data for each algorithm\n",
        "ppo_monitor_df = pd.read_csv(os.path.join(ppo_log_dir, 'monitor.csv'),skiprows=[0],  index_col=None)\n",
        "sac_monitor_df = pd.read_csv(os.path.join(sac_log_dir, 'monitor.csv'), skiprows=[0], index_col=None)\n",
        "ddpg_monitor_df = pd.read_csv(os.path.join(ddpg_log_dir, 'monitor.csv'), skiprows=[0], index_col=None)\n",
        "td3_df = pd.read_csv('./benchmarks/logs/TD3_log.csv', index_col=None)\n",
        "\n",
        "# Plot learning curves\n",
        "\n",
        "# Rewards vs Episodes\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(ppo_monitor_df['r'], label='PPO')\n",
        "plt.plot(sac_monitor_df['r'], label='SAC')\n",
        "plt.plot(ddpg_monitor_df['r'], label='DDPG')\n",
        "plt.plot(td3_df['r'], label='TD3')\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Episode Reward')\n",
        "plt.title('Learning Curves: Rewards vs Episodes')\n",
        "plt.xticks(rotation=90)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Episode length vs Episodes\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(ppo_monitor_df['l'], label='PPO')\n",
        "plt.plot(sac_monitor_df['l'], label='SAC')\n",
        "plt.plot(ddpg_monitor_df['l'], label='DDPG')\n",
        "plt.plot(td3_df['l'], label='TD3')\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Episode length')\n",
        "plt.title('Learning Curves: Episode len vs Episodes')\n",
        "plt.xticks(rotation=90)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Rewards + Episode len vs Episodes\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(ppo_monitor_df['l'] + ppo_monitor_df['r'], label='PPO')\n",
        "plt.plot(sac_monitor_df['l'] + sac_monitor_df['r'], label='SAC')\n",
        "plt.plot(ddpg_monitor_df['l'] + ddpg_monitor_df['r'], label='DDPG')\n",
        "plt.plot(td3_df['l'] + td3_df['r'], label='TD3')\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Episode len + Rewards')\n",
        "plt.title('Learning Curves: Episode len + Rewards vs Episodes')\n",
        "plt.xticks(rotation=90)  # Rotate the y-axis labels by 45 degrees\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K9mXAOBAa1ie"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}